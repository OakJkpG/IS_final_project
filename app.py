# app.py
import streamlit as st
import pandas as pd
import numpy as np
import os
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ layout ‡∏Ç‡∏≠‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö
st.set_page_config(page_title="ML&NN IS_Project", layout="wide")

# ----- Custom CSS ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö -----
st.markdown(
    """
    <style>
    /* ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏•‡∏∞ font */
    body {
        background-color: #f8f9fa;
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    }
    /* ‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á‡∏õ‡∏∏‡πà‡∏° */
    .stButton>button {
        background-color: #4CAF50;
        color: white;
        border: none;
        border-radius: 8px;
        padding: 8px 16px;
        font-size: 16px;
    }
    /* ‡∏ï‡∏Å‡πÅ‡∏ï‡πà‡∏á‡πÅ‡∏ó‡πá‡∏ö */
    .css-1outpf7 {
        background-color: #ffffff;
        border-bottom: 2px solid #dee2e6;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ----- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV -----
@st.cache_data
def load_data(file_path):
    return pd.read_csv(file_path)

# ----- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataset1 (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Demo ML) -----
def prepare_dataset1(df):
    # ‡πÄ‡∏ï‡∏¥‡∏° missing values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    df['Feature_A'].fillna(df['Feature_A'].mean(), inplace=True)
    df['Feature_B'].fillna(df['Feature_B'].mean(), inplace=True)
    # ‡πÄ‡∏ï‡∏¥‡∏° missing values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ categorical ‡∏î‡πâ‡∏ß‡∏¢ mode
    df['Feature_C'].fillna(df['Feature_C'].mode()[0], inplace=True)
    # ‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ Feature_C
    le = LabelEncoder()
    df['Feature_C_enc'] = le.fit_transform(df['Feature_C'])
    return df, le

# ----- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÑ‡∏ß‡πâ -----
@st.cache_resource
def load_nn_model():
    model = load_model('models/neural_network.h5')
    return model

# ----- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î pre-saved label encoders (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) -----
@st.cache_resource
def load_label_encoders():
    with open('models/label_encoders.pkl', 'rb') as f:
        encoders = pickle.load(f)
    return encoders

# ----- ‡∏™‡∏£‡πâ‡∏≤‡∏á Navigation ‡πÅ‡∏ö‡∏ö‡πÅ‡∏ó‡πá‡∏ö‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô -----
tabs = st.tabs(["üè† Home", "üìò ML Explanation", "üìô NN Explanation", "ü§ñ ML Model Demo", "üß† NN Model Demo"])

# ==========================================================
# Tab 1: Overview
# ==========================================================
with tabs[0]:
    st.title("Overview ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ")
    st.write("""
        **‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ:**  
        ‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡πÅ‡∏•‡∏∞ Neural Network ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå  
        ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å:
        - **Dataset1:** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏î‡πâ‡∏ß‡∏¢ **Decision Tree** ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ **K-Means**
        - **Dataset2:** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• **Neural Network** ‡πÉ‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Regression  
        
        **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢:**  
        - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå  
        - ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ  
        - ‡∏™‡∏≤‡∏ò‡∏¥‡∏ï‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏ß‡πá‡∏ö‡πÅ‡∏≠‡∏õ‡∏û‡∏•‡∏¥‡πÄ‡∏Ñ‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Streamlit  
    """)
    #st.image("https://via.placeholder.com/1000x300.png?text=Machine+Learning+%26+Neural+Network+Project", use_column_width=True)

# ==========================================================
# Tab 2: Machine Learning (‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ ML)
# ==========================================================
with tabs[1]:
    st.title("Machine Learning: Data Preparation & Model Development")
    st.write("""
        **‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**  
        - **Dataset1:** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values ‡πÉ‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó  
        - ‡πÄ‡∏ï‡∏¥‡∏° missing values ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç) ‡πÅ‡∏•‡∏∞ mode (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical)  
        - ‡πÉ‡∏ä‡πâ LabelEncoder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç  
        
        **‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°:**  
        - **Decision Tree:**  
            - ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≤‡∏Ç‡∏≤‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ feature  
            - ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à  
        - **K-Means Clustering:**  
            - ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ô  
            - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ target ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å (unsupervised learning)  
        
        **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤:**  
        1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå  
        2. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö supervised learning)  
        3. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á hyperparameters  
        4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  
    """)
    st.subheader("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataset1")
    df1_sample = load_data('data/dataset1.csv').head(10)
    st.dataframe(df1_sample)

# ==========================================================
# Tab 3: Neural Network (‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤ NN)
# ==========================================================
with tabs[2]:
    st.title("Neural Network: Data Preparation & Model Development")
    st.write("""
        **‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:**  
        - **Dataset2:** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values  
        - ‡πÄ‡∏ï‡∏¥‡∏° missing values ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Sensor ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå
        
        **‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏≠‡∏á Neural Network:**  
        - ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÅ‡∏£‡∏á‡∏ö‡∏±‡∏ô‡∏î‡∏≤‡∏•‡πÉ‡∏à‡∏à‡∏≤‡∏Å‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏™‡∏°‡∏≠‡∏á  
        - ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå Dense (fully-connected) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
        - ‡πÉ‡∏ä‡πâ Dropout ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ overfitting  
        - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Regression ‡∏à‡∏∞‡∏°‡∏µ Output layer ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß  
        
        **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤:**  
        1. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test  
        2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå  
        3. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ optimizer (‡πÄ‡∏ä‡πà‡∏ô Adam) ‡πÅ‡∏•‡∏∞ loss function (‡πÄ‡∏ä‡πà‡∏ô MSE)  
        4. ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  
    """)
    st.subheader("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataset2")
    df2_sample = load_data('data/dataset2.csv').head(10)
    st.dataframe(df2_sample)

# ==========================================================
# Tab 4: Demo Machine Learning (Decision Tree & K-Means ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô)
# ==========================================================
with tabs[3]:
    st.title("Demo Machine Learning")
    
    st.markdown("## Decision Tree Classification")
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Decision Tree")
        dt_max_depth = st.slider("Max Depth", min_value=1, max_value=20, value=5, step=1)
        dt_min_samples_split = st.slider("Min Samples Split", min_value=2, max_value=20, value=2, step=1)
    
    with col2:
        st.markdown("### ‡∏õ‡πâ‡∏≠‡∏ô‡∏Ñ‡πà‡∏≤ Feature ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢")
        feature_A = st.number_input("Feature_A", value=50.0, key="dt_feature_A")
        feature_B = st.number_input("Feature_B", value=50.0, key="dt_feature_B")
        feature_C = st.selectbox("Feature_C", options=["Low", "Medium", "High"], key="dt_feature_C")
    
    # ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Dataset1
    df1 = load_data('data/dataset1.csv')
    df1, le = prepare_dataset1(df1)
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á target encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Decision Tree
    le_target = LabelEncoder()
    df1['Target_enc'] = le_target.fit_transform(df1['Target'])
    
    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å Decision Tree
    X = df1[['Feature_A', 'Feature_B', 'Feature_C_enc']]
    y = df1['Target_enc']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # ‡∏ù‡∏∂‡∏Å Decision Tree ‡∏î‡πâ‡∏ß‡∏¢ hyperparameters ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    dt_model = DecisionTreeClassifier(max_depth=dt_max_depth, min_samples_split=dt_min_samples_split, random_state=42)
    dt_model.fit(X_train, y_train)
    
    if st.button("‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Decision Tree", key="btn_dt"):
        feature_C_enc = le.transform([feature_C])[0]
        input_features = np.array([[feature_A, feature_B, feature_C_enc]])
        prediction = dt_model.predict(input_features)
        predicted_class = le_target.inverse_transform(prediction)[0]
        st.success(f"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó (Decision Tree): {predicted_class}")
    
    st.markdown("---")
    # decision tree
    from sklearn.tree import plot_tree
    import matplotlib.pyplot as plt

    # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ dt_model ‡∏Ñ‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• Decision Tree ‡∏ó‡∏µ‡πà‡∏ù‡∏∂‡∏Å‡πÑ‡∏ß‡πâ ‡πÅ‡∏•‡∏∞ le_target ‡∏Ñ‡∏∑‡∏≠ LabelEncoder ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö target
    fig, ax = plt.subplots(figsize=(12, 8))
    plot_tree(dt_model, filled=True, feature_names=['Feature_A', 'Feature_B', 'Feature_C_enc'], class_names=le_target.classes_)
    st.pyplot(fig)

    st.markdown("## K-Means Clustering")
    n_clusters = st.slider("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå (Clusters)", min_value=2, max_value=10, value=3, step=1, key="km_clusters")
    if st.button("‡∏£‡∏±‡∏ô K-Means", key="btn_km"):
        X_km = df1[['Feature_A', 'Feature_B', 'Feature_C_enc']]
        kmeans_model = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans_model.fit_predict(X_km)
        df1['Cluster'] = clusters
        
        st.success(f"K-Means ‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô {n_clusters} ‡∏Ñ‡∏•‡∏±‡∏™‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡πâ‡∏ß")
        
        fig, ax = plt.subplots(figsize=(8,6))
        sns.scatterplot(x='Feature_A', y='Feature_B', hue='Cluster', data=df1, palette='viridis', ax=ax)
        ax.set_title("K-Means Clustering ‡∏Ç‡∏≠‡∏á Dataset1")
        st.pyplot(fig)

# ==========================================================
# Tab 5: Demo Neural Network
# ==========================================================
with tabs[4]:
    st.title("Demo Neural Network for Regression")
    st.write("‡∏õ‡πâ‡∏≠‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏ã‡∏ô‡πÄ‡∏ã‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤ Output ‡∏î‡πâ‡∏ß‡∏¢ Neural Network")
    
    sensor1 = st.number_input("Sensor1", value=20.0, key="nn_sensor1")
    sensor2 = st.number_input("Sensor2", value=50.0, key="nn_sensor2")
    sensor3 = st.number_input("Sensor3", value=100.0, key="nn_sensor3")
    
    nn_model = load_nn_model()
    
    if st.button("‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ Neural Network"):
        input_data = np.array([[sensor1, sensor2, sensor3]])
        pred_output = nn_model.predict(input_data)
        st.success(f"‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ Output: {pred_output[0][0]:.2f}")
    
    st.subheader("Distribution ‡∏Ç‡∏≠‡∏á Output ‡πÉ‡∏ô Dataset2")
    df2 = load_data('data/dataset2.csv')
    fig, ax = plt.subplots(figsize=(8,6))
    sns.histplot(df2['Output'], kde=True, ax=ax)
    ax.set_title("Distribution of Output in Dataset2")
    st.pyplot(fig)
